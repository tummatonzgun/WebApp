import pandas as pd
import os
import glob
import numpy as np
from pathlib import Path
import time
from datetime import datetime  

BASE_DIR = os.path.dirname(os.path.abspath(__file__))


# ---------- 1. TXT ‚Üí Excel ----------

def find_input_files(input_pattern: str):
    if os.path.isdir(input_pattern):
        files = glob.glob(os.path.join(input_pattern, "*.txt")) + \
        glob.glob(os.path.join(input_pattern, "*.TXT"))
        files = list(set(os.path.abspath(f) for f in files))
    else:
        files = glob.glob(input_pattern)
    return files

def load_and_parse_file(input_file: str) -> pd.DataFrame:
    try:
        with open(input_file, 'r', encoding='latin-1') as file:
            lines = file.readlines()
    except Exception as e:
        print(f"Error reading {input_file}: {e}")
        return pd.DataFrame()
    rows = []
    max_values_len = 0
    for line in lines:
        parts = line.strip().split('\t')
        if len(parts) < 3:
            continue
        timestamp, data_type, data_values = parts[0], parts[1], parts[2]
        try:
            date_part, time_part = timestamp.split(' ')
            time_part = time_part.replace('AM', '').replace('PM', '').strip()
        except ValueError:
            continue
        values = data_values.split(',')
        if len(values) > max_values_len:
            max_values_len = len(values)
        row = [date_part, time_part, data_type] + values
        rows.append(row)
    if not rows:
        return pd.DataFrame()
    max_len = 6 + max_values_len
    for row in rows:
        row += [''] * (max_len - len(row))
    columns = ['date', 'time', 'step', 'frame', 'G', 'No_strip'] + [f'value_{i}' for i in range(1, max_values_len + 1)]
    df = pd.DataFrame(rows, columns=columns)
    pattern = r'(FU|FR|FA|FW|FN|FJ|F1|F2|F3|F4|F5|F6|F7|F8|F9|F0)(\w{4})'
    df['frame'] = df['frame'].astype(str)
    df['frame'] = df['frame'].str.extract(pattern).fillna('').agg(''.join, axis=1)
    return df

def extract_pro_and_speed(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame()
    df_pro = df[df['step'] == 'PRO'].copy()
    if df_pro.empty:
        return pd.DataFrame()
    df_pro['speed'] = None
    for idx in df_pro.index:
        speed_value = None
        pos = df.index.get_loc(idx)
        for j in range(pos + 1, len(df)):
            if df.loc[df.index[j], 'step'] == 'CUC':
                if 'value_5' in df.columns and len(df.columns) > df.columns.get_loc('value_5'):
                    speed_value = df.loc[df.index[j], 'value_5']
                break
        df_pro.at[idx, 'speed'] = speed_value
    df_pro['speed'] = pd.to_numeric(df_pro['speed'], errors='coerce')
    df_pro['speed'] = df_pro['speed'] / 10 / 25.4
    df_pro['speed'] = df_pro['speed'].apply(lambda x: int(x) if x % 1 == 0 else round(x, 2))
    return df_pro

def mark_errors(df: pd.DataFrame, df_pro: pd.DataFrame) -> pd.DataFrame:
    if df.empty or df_pro.empty:
        return df_pro
    df_pro['MC'] = None
    pro_indices = df.index[df['step'] == 'PRO'].tolist()
    for i in range(1, len(pro_indices)):
        current_idx = pro_indices[i]
        prev_idx = pro_indices[i - 1]
        start_idx = min(prev_idx, current_idx)
        end_idx = max(prev_idx, current_idx)
        df_slice = df.loc[start_idx:end_idx]
        error_steps = ['ERRSET', 'ERRRCV', 'ERRCLR', 'DMC', 'DMW']
        has_error = df_slice['step'].isin(error_steps).any()
        if has_error and prev_idx in df_pro.index:
            df_pro.at[prev_idx, 'MC'] = 'MC error'
    return df_pro

def insert_blank_rows(df_pro: pd.DataFrame) -> pd.DataFrame:
    if df_pro.empty:
        return df_pro
    new_rows = []
    for i in range(len(df_pro)):
        row = df_pro.iloc[i]
        new_rows.append(row)
        try:
            first_strip = float(row['No_strip']) == 1
        except (ValueError, TypeError):
            first_strip = False
        if first_strip:
            empty_row = pd.Series([None] * len(df_pro.columns), index=df_pro.columns)
            new_rows.append(empty_row)
    df_with_blank = pd.DataFrame(new_rows).reset_index(drop=True)
    return df_with_blank

def calculate_time_diff(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')
    df['minute'] = df['datetime'] - df['datetime'].shift(-1)
    df['seconds'] = df['minute'].dt.total_seconds()
    df = df[df['seconds'].isna() | ((df['seconds'] >= 0) & (df['seconds'] <= 86400))]
    df['minute'] = df['minute'].astype(str)
    df.loc[df['minute'] == 'NaT', 'minute'] = ''
    df['minute'] = df['minute'].str.replace('0 days ', '')
    df = df.drop(columns=['datetime'])
    return df

def assign_subgroups_and_insert_empty_rows(df, column_strip='No_strip', frame_group='frame'):
    subgroup_id = 0
    subgroups = []
    prev_val = None
    for val in df[column_strip]:
        if pd.isna(val):
            subgroups.append(np.nan)
            prev_val = None
            continue
        if prev_val is None:
            subgroup_id += 1
        elif val > prev_val:
            subgroup_id += 1
        subgroups.append(subgroup_id)
        prev_val = val
    df['subgroup_id'] = subgroups
    result_rows = []
    subgroup_keys = df['subgroup_id'].dropna().unique()
    for group in subgroup_keys:
        group_df = df[df['subgroup_id'] == group].reset_index(drop=True)
        result_rows.append(group_df.iloc[[0]])
        for i in range(1, len(group_df)):
            prev_frame = group_df.loc[i - 1, frame_group]
            curr_frame = group_df.loc[i, frame_group]
            if prev_frame != curr_frame:
                empty_row = pd.DataFrame({col: [np.nan] for col in df.columns})
                result_rows.append(empty_row)
            result_rows.append(group_df.iloc[[i]])
        empty_row = pd.DataFrame({col: [np.nan] for col in df.columns})
        result_rows.append(empty_row)
    result_df = pd.concat(result_rows, ignore_index=True).reset_index(drop=True)
    return result_df

def mark_outlier_subgroups(df, subgroup_col='subgroup_id', no_strip_col='No_strip'):
    outlier_groups = []
    for subgroup, group_df in df.groupby(subgroup_col):
        if 1 not in group_df[no_strip_col].values:
            outlier_groups.append(subgroup)
    df['outlier_subgroup'] = df[subgroup_col].isin(outlier_groups)
    return df

def detect_outliers_combined(df, group_col='frame', value_col='seconds', no_strip_col='No_strip',
                             iqr_factor=1, zscore_threshold=2, min_diff_seconds=90):
    df['is_outlier'] = False
    df_filtered = df[~((df[no_strip_col] == 2) & (df[no_strip_col].shift(-1) == 1))]
    for group_name, group in df_filtered.groupby(group_col):
        values = group[value_col].dropna().values
        if len(values) == 0:
            continue
        median = np.median(values)
        q1 = np.percentile(values, 25)
        q3 = np.percentile(values, 75)
        iqr = q3 - q1
        upper_bound = q3 + iqr_factor * iqr
        mean = np.mean(values)
        std = np.std(values)
        for idx in group.index:
            val = df.loc[idx, value_col]
            if pd.isna(val):
                continue
            if val > upper_bound and abs(val - median) > min_diff_seconds:
                df.at[idx, 'is_outlier'] = True
            elif std > 0:
                z_score = (val - mean) / std
                if z_score > zscore_threshold and abs(val - mean) > min_diff_seconds:
                    df.at[idx, 'is_outlier'] = True
    return df

def add_avg_exclude_outliers_by_frame(
    df, 
    value_col='seconds', 
    group_col='frame', 
    outlier_col='is_outlier',
    outlier_subgroup_col='outlier_subgroup',
    outlier_mc='MC'
):
    df['avg_ex_outliers'] = pd.NA
    df['count_avg'] = pd.NA
    df['count_outliers'] = pd.NA
    for frame_val in df[group_col].dropna().unique():
        group_df = df[df[group_col] == frame_val]
        good_values = group_df[(group_df[outlier_col] != True) & (group_df[outlier_subgroup_col] != True) & (group_df[outlier_mc] != 'MC error')][value_col].dropna()
        if len(good_values) < 5:
            continue
        avg_val = good_values.mean()
        count_avg_val = len(good_values)
        count_all = group_df[value_col].count()
        count_outliers = count_all - count_avg_val
        idx = df[df[group_col] == frame_val].index
        if len(idx) > 0:
            first_idx = idx[0]
            df.at[first_idx, 'avg_ex_outliers'] = round(avg_val, 2)
            df.at[first_idx, 'count_avg'] = count_avg_val
            df.at[first_idx, 'count_outliers'] = count_outliers
    return df

def summarize_by_frame(df):
    summary = df.groupby(['frame', 'speed']).agg({
        'sec/strip': 'first',
    }).reset_index()
    return summary

def process_single_file_complete(input_file: str, output_dir: str):
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {input_file}")
    input_path = Path(input_file)
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_path / f"{input_path.stem}_{timestamp}.xlsx"
    
    try:
        df = load_and_parse_file(input_file)
        if df.empty:
            return False, f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {input_file}"
        
        df_pro = extract_pro_and_speed(df)
        if df_pro.empty:
            return False, f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PRO ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {input_file}"
        
        df_pro = mark_errors(df, df_pro)
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        available_value_cols = [col for col in df_pro.columns if col.startswith('value_')]
        value_cols = available_value_cols[:1] if available_value_cols else []
        selected_cols = ['date', 'time', 'step', 'package', 'frame', 'No_strip'] + value_cols + ['speed','MC']
        existing_cols = [col for col in selected_cols if col in df_pro.columns]
        df_pro = df_pro[existing_cols]
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_with_blank = insert_blank_rows(df_pro)
        df_time = calculate_time_diff(df_with_blank)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        for col in ['frame', 'speed','value_1']:
            if col in df_time.columns:
                if col == 'frame':
                    df_time[col] = df_time[col].astype(str).str.strip()
                else:
                    df_time[col] = pd.to_numeric(df_time[col], errors='coerce')
        
        if 'No_strip' in df_time.columns:
            df_time['No_strip'] = pd.to_numeric(df_time['No_strip'], errors='coerce')
        
        df_filtered = df_time[df_time['frame'].notna()]
        if df_filtered.empty:
            return False, f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• frame ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {input_file}"
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_analyzed = assign_subgroups_and_insert_empty_rows(df_filtered, 'No_strip', 'frame')
        df_analyzed = mark_outlier_subgroups(df_analyzed, 'subgroup_id', 'No_strip')
        df_analyzed = detect_outliers_combined(df_analyzed, 'frame', 'seconds', 'No_strip')
        df_analyzed = add_avg_exclude_outliers_by_frame(df_analyzed, value_col='seconds', group_col='frame')
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Error columns
        if 'outlier_subgroup' in df_analyzed.columns and 'is_outlier' in df_analyzed.columns and 'MC' in df_analyzed.columns:
            df_analyzed['Error'] = (df_analyzed['outlier_subgroup'] | df_analyzed['is_outlier'] | (df_analyzed['MC'] == 'MC error'))
        elif 'outlier_subgroup' in df_analyzed.columns and 'is_outlier' in df_analyzed.columns:
            df_analyzed['Error'] = df_analyzed['outlier_subgroup'] | df_analyzed['is_outlier']
        else:
            df_analyzed['Error'] = False
        
        # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        df_analyzed.drop(columns=['outlier_subgroup', 'is_outlier','MC'], inplace=True, errors='ignore')
        df_analyzed['Error'] = df_analyzed['Error'].apply(lambda x: "MC ERROR" if x else "")
        df_analyzed.drop(columns=['subgroup_id'], inplace=True, errors='ignore')
        df_analyzed['sec/strip'] = df_analyzed['avg_ex_outliers']
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Summary
        summary = summarize_by_frame(df_analyzed)
        df_final = df_analyzed.drop(columns=['avg_ex_outliers'])
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel
        with pd.ExcelWriter(output_file) as writer:
            df_final.to_excel(writer, index=False, sheet_name='Processed_Data')
            summary.to_excel(writer, index=False, sheet_name='Summary')
        
        return True, str(output_file)
        
    except Exception as e:
        return False, f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {input_file}: {str(e)}"

def process_multiple_files_complete(input_pattern: str, output_dir: str):
    files = find_input_files(input_pattern)
    if not files:
        print(f" ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö pattern: {input_pattern}")
        return
    print(f" ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(files)} ‡πÑ‡∏ü‡∏•‡πå")
    print("=" * 60)
    successful = 0
    failed = 0
    start_time = time.time()
    for i, file_path in enumerate(files, 1):
        print(f"[{i}/{len(files)}] ", end="")
        success, message = process_single_file_complete(file_path, output_dir)
        if success:
            print(f" ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {message}")
            successful += 1
        else:
            print(f" ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {message}")
            failed += 1
    end_time = time.time()
    print("\n" + "=" * 60)
    print(f" ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {end_time - start_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
    print(f" ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {successful} ‡πÑ‡∏ü‡∏•‡πå, ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß {failed} ‡πÑ‡∏ü‡∏•‡πå")

# ---------- 2. ‡∏£‡∏ß‡∏° Summary ----------

def load_sec_strip_by_frame(filepath, sheet_name='Processed_Data'):
    try:
        df = pd.read_excel(filepath, sheet_name=sheet_name)
    except ValueError:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏µ‡∏ó Processed_Data ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô Sheet1
        df = pd.read_excel(filepath, sheet_name='Sheet1')
    if not all(col in df.columns for col in ['frame', 'speed', 'sec/strip']):
        raise ValueError("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£")
    df['frame'] = df['frame'].astype(str)
    df['speed'] = pd.to_numeric(df['speed'], errors='coerce')
    df['sec/strip'] = pd.to_numeric(df['sec/strip'], errors='coerce')
    df = df[df['sec/strip'].notna() & df['speed'].notna()]
    return df

def summarize_sec_strip(files_folder, file_list):
    data = {}
    for filename in file_list:
        filepath = os.path.join(files_folder, filename)
        try:
            df = load_sec_strip_by_frame(filepath)
            summary = df.groupby(['frame', 'speed'])['sec/strip'].mean()
            summary.index = summary.index.map(lambda x: f"{x[0]}_speed{x[1]}")
            file_key = os.path.splitext(filename)[0]
            data[file_key] = summary
        except Exception as e:
            print(f"‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå {filename} : {e}")
            continue
    result_df = pd.DataFrame(data)
    result_df = result_df.sort_index()
    return result_df

def save_summary(df, output_path):
    df.index.name = "FRAME_STOCK"
    df.to_excel(output_path, index=True)
    print(f"‚úÖ Saved comparison summary to: {output_path}")

# ---------- 3. Export CSV ----------

def remove_outliers(data):
    if not data:
        return []
    arr = np.array(data)
    q1 = np.percentile(arr, 25)
    q3 = np.percentile(arr, 75)
    iqr = q3 - q1
    upper_bound = q3 + 1.5 * iqr
    filtered = arr[(arr <= upper_bound)]
    return filtered.tolist()

def filtered_mean(lst):
    filtered = remove_outliers(lst)
    if len(filtered) == 0:
        return float('nan')
    return sum(filtered) / len(filtered)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î mapping ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Package group ‚Üí Lead frame
MAPPING = {
    ('QFN', '5.0'): 'CU',
    ('QFN', '4.0'): 'PPF',
}

def analyze_and_export_csv(summary_path, package_path, output_csv):
    df = pd.read_excel(summary_path)
    df2 = pd.read_excel(package_path)
    df['non_null_values'] = df.loc[:, df.columns != 'FRAME_STOCK'].apply(
        lambda row: row.dropna().tolist(), axis=1)
    df = df[['FRAME_STOCK', 'non_null_values']]
    df['TIME/STRIP'] = df['non_null_values'].apply(filtered_mean)
    df = df[['FRAME_STOCK', 'TIME/STRIP']]
    df['SPEED'] = df['FRAME_STOCK'].astype(str).str[-3:]
    df['X'] = df['FRAME_STOCK'].astype(str).str[0:6]
    df = df[['X', 'SPEED', 'TIME/STRIP', 'FRAME_STOCK']]
    df = df.drop(columns='FRAME_STOCK')
    df['TIME/STRIP'] = df['TIME/STRIP'].round(2)
    df.rename(columns={'X': 'FRAME_STOCK'}, inplace=True)
    df.rename(columns={'SPEED': 'SPEED (IPS)'},inplace=True)
    df_merged = pd.merge(df, df2[['FRAME_STOCK', 'PACKAGE_CODE']], on='FRAME_STOCK', how='left')
    df_merged.to_csv(output_csv, index=False)
    print(f"‚úÖ Exported summary CSV: {output_csv}")

def analyze_and_export_csv_from_df(summary_df, package_path, output_csv):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV
    """
    print("üìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å CSV...")
    
    df = summary_df.reset_index()
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå index ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 'FRAME_STOCK'
    if df.columns[0] != 'FRAME_STOCK':
        df = df.rename(columns={df.columns[0]: 'FRAME_STOCK'})
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• package
    print(f"üìÅ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• package ‡∏à‡∏≤‡∏Å: {package_path}")
    df2 = pd.read_excel(package_path)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå package
    print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå package...")
    required_cols = ['FRAME_STOCK', 'PACKAGE_CODE', 'Package size ', 'Package group', 'Lead frame type by frame stock ', 'Unit/strip']
    available_cols = ['FRAME_STOCK']  # FRAME_STOCK ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ
    
    for col in required_cols[1:]:  # ‡∏Ç‡πâ‡∏≤‡∏° FRAME_STOCK
        if col in df2.columns:
            available_cols.append(col)
            print(f"   ‚úÖ ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {col}")
        else:
            print(f"   ‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {col}")
    
    print(f"üìä ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ merge: {available_cols}")
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    df['non_null_values'] = df.loc[:, df.columns != 'FRAME_STOCK'].apply(
        lambda row: row.dropna().tolist(), axis=1)
    df = df[['FRAME_STOCK', 'non_null_values']]
    df['TIME/STRIP'] = df['non_null_values'].apply(filtered_mean)
    df = df[['FRAME_STOCK', 'TIME/STRIP']]
    df['SPEED (IPS)'] = df['FRAME_STOCK'].astype(str).str[-3:]
    df['X'] = df['FRAME_STOCK'].astype(str).str[0:6]
    df = df[['X', 'SPEED (IPS)', 'TIME/STRIP', 'FRAME_STOCK']]
    df = df.drop(columns='FRAME_STOCK')
    df['TIME/STRIP'] = df['TIME/STRIP'].round(2)
    df.rename(columns={'X': 'FRAME_STOCK'}, inplace=True)
    
    # Merge ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• package
    print("üîó ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö package data...")
    df_merged = pd.merge(
        df,
        df2[available_cols],
        on='FRAME_STOCK',
        how='left'
    )
    
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ MAPPING (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)
    if 'Lead frame type by frame stock' in df_merged.columns and 'Package group' in df_merged.columns:
        print("üîß ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Lead frame ‡∏ï‡∏≤‡∏° mapping...")
        df_merged['Lead frame type by frame stock'] = df_merged.apply(
            lambda row: MAPPING.get((str(row['Package group']), str(row['SPEED (IPS)'])), row['Lead frame type by frame stock']),
            axis=1
        )
    else:
        print("‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏° MAPPING ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Process (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Package group)
    if 'Package group' in df_merged.columns:
        print("‚öôÔ∏è ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Process...")
        df_merged['Process'] = None
        df_merged['Package group'] = df_merged['Package group'].astype(str).str.strip().str.upper()
        # ‡πÅ‡∏õ‡∏•‡∏á SPEED ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        df_merged['SPEED (IPS)'] = pd.to_numeric(df_merged['SPEED (IPS)'], errors='coerce')

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Process ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
        choices = [
           (df_merged['SPEED (IPS)'] == 5) & (df_merged['Package group'] == 'SLP'),
           (df_merged['SPEED (IPS)'] == 3) & (df_merged['Package group'] == 'SLP')
        ]
        answer = ['Full Cut', 'Step Cut']
        df_merged['Process'] = np.select(choices, answer, default=None)
    else:
        print("‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏° Process ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Package group")
        # ‡πÅ‡∏õ‡∏•‡∏á SPEED ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢
        df_merged['SPEED (IPS)'] = pd.to_numeric(df_merged['SPEED (IPS)'], errors='coerce')
    
    # ‚úÖ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ group_and_average_across_frames_unique_frame
    print("üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡πâ‡∏≤‡∏° frame...")
    df_final = group_and_average_across_frames_unique_frame(df_merged)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV: {output_csv}")
    df_final.to_csv(output_csv, index=False)
    print(f"‚úÖ Exported summary CSV: {output_csv}")
    
    return df_final

def group_and_average_across_frames_unique_frame(df_merged):
    """
    ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡πâ‡∏≤‡∏° Frame - ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    """
    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢...")
    
    # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° (‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
    potential_grouping_cols = [
        'Package size ',
        'Package group',
        'Lead frame type by frame stock ',
        'Unit/strip',
        'SPEED (IPS)'
    ]
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    grouping_cols = [col for col in potential_grouping_cols if col in df_merged.columns]
    
    # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ SPEED (IPS) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°
    if 'SPEED (IPS)' not in grouping_cols:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå SPEED (IPS) ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°")
        return df_merged
    
    print(f"üìä ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°: {grouping_cols}")
    missing_cols = [col for col in potential_grouping_cols if col not in df_merged.columns]
    if missing_cols:
        print(f"‚ö†Ô∏è  ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {missing_cols}")
    
    print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {df_merged.shape[0]} ‡πÅ‡∏ñ‡∏ß")
    
    # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏ï‡∏≤‡∏° FRAME_STOCK
    df_unique = df_merged.drop_duplicates(subset=['FRAME_STOCK'])
    print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö duplicate: {df_unique.shape[0]} ‡πÅ‡∏ñ‡∏ß")
    
    group_avg_map = {}
    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
    group_before_map = {}  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏î
    group_after_map = {}   # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î
    total_groups = 0
    processed_groups = 0
    total_outliers_removed = 0

    print("üîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    print("=" * 80)
    
    for group_key, group_df in df_unique.groupby(grouping_cols):
        total_groups += 1
        values = group_df['TIME/STRIP'].dropna().tolist()
        
        # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á)
        if isinstance(group_key, tuple):
            group_name_parts = []
            for i, col in enumerate(grouping_cols):
                if i < len(group_key):
                    if col == 'SPEED (IPS)':
                        group_name_parts.append(f"Speed:{group_key[i]}")
                    elif col == 'Package size ':
                        group_name_parts.append(f"Size:{group_key[i]}")
                    elif col == 'Package group':
                        group_name_parts.append(f"Group:{group_key[i]}")
                    elif col == 'Unit/strip':
                        group_name_parts.append(f"Unit:{group_key[i]}")
                    elif col == 'Lead frame type by frame stock':
                        group_name_parts.append(f"Lead:{group_key[i]}")
                    else:
                        group_name_parts.append(str(group_key[i]))
            group_name = " | ".join(group_name_parts)
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            if grouping_cols[0] == 'SPEED (IPS)':
                group_name = f"Speed:{group_key}"
            else:
                group_name = f"{grouping_cols[0]}:{group_key}"
        
        if len(values) < 2:
            print(f"‚ùå ‡∏Å‡∏•‡∏∏‡πà‡∏°: {group_name}")
            print(f"   üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠: {len(values)} ‡∏Ñ‡πà‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏Ñ‡πà‡∏≤)")
            print(f"   üìã ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ: {values}")
            
            # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
            group_before_map[group_key] = len(values)
            group_after_map[group_key] = 0
            print()
            continue
            
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• outliers
        q1 = np.percentile(values, 25)
        q3 = np.percentile(values, 75)
        iqr = q3 - q1
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
        filtered = [v for v in values if lower <= v <= upper]
        outliers = [v for v in values if v < lower or v > upper]
        
        # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
        group_before_map[group_key] = len(values)
        group_after_map[group_key] = len(filtered)
        
        if filtered:
            avg_val = round(np.mean(filtered), 2)
            group_avg_map[group_key] = avg_val
            processed_groups += 1
            total_outliers_removed += len(outliers)
            
            print(f"‚úÖ ‡∏Å‡∏•‡∏∏‡πà‡∏°: {group_name}")
            print(f"   üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(values)} ‡∏Ñ‡πà‡∏≤")
            print(f"   üìà ‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏Å‡∏ï‡∏¥: {round(lower, 2)} - {round(upper, 2)}")
            print(f"   ‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {len(filtered)} ‡∏Ñ‡πà‡∏≤ ‚Üí {filtered}")
            
            if outliers:
                print(f"   ‚ùå Outliers ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å: {len(outliers)} ‡∏Ñ‡πà‡∏≤ ‚Üí {outliers}")
                print(f"   üìä ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î: {round(len(outliers)/len(values)*100, 1)}%")
            else:
                print(f"   ‚ú® ‡πÑ‡∏°‡πà‡∏°‡∏µ outliers (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏Å‡∏ï‡∏¥)")
                
            print(f"   üéØ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {avg_val}")
            print(f"   üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(values)}/{len(filtered)} (‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏î/‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î)")
            print()
        else:
            print(f"‚ö†Ô∏è  ‡∏Å‡∏•‡∏∏‡πà‡∏°: {group_name}")
            print(f"   üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(values)} ‡∏Ñ‡πà‡∏≤ ‚Üí {values}")
            print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô outliers)")
            print(f"   üìà ‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏Å‡∏ï‡∏¥: {round(lower, 2)} - {round(upper, 2)}")
            print(f"   üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(values)}/0 (‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏î/‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î)")
            print()

    print("=" * 80)
    print(f"üìà ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:")
    print(f"   üî¢ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_groups} ‡∏Å‡∏•‡∏∏‡πà‡∏°")
    print(f"   ‚úÖ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ: {processed_groups} ‡∏Å‡∏•‡∏∏‡πà‡∏°")
    print(f"   ‚ùå ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ: {total_groups - processed_groups} ‡∏Å‡∏•‡∏∏‡πà‡∏°")
    print(f"   üóëÔ∏è  Outliers ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_outliers_removed} ‡∏Ñ‡πà‡∏≤")
    print(f"   üìä ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {round(processed_groups/total_groups*100, 1)}%")
    print("=" * 80)

    # ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏î
    def assign_before_count(row):
        key = tuple(row[col] for col in grouping_cols)
        return group_before_map.get(key, 0)

    # ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î
    def assign_after_count(row):
        key = tuple(row[col] for col in grouping_cols)
        return group_after_map.get(key, 0)

    # ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
    def assign_avg(row):
        key = tuple(row[col] for col in grouping_cols)
        original_value = row['TIME/STRIP']
        new_value = group_avg_map.get(key, original_value)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if pd.notna(original_value) and pd.notna(new_value) and abs(original_value - new_value) > 0.01:
            change_type = "üìà" if new_value > original_value else "üìâ"
            diff = abs(new_value - original_value)
            before_count = group_before_map.get(key, 0)
            after_count = group_after_map.get(key, 0)
            print(f"   {change_type} {row['FRAME_STOCK']}: {original_value} ‚Üí {new_value} (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô {round(diff, 2)}) [{before_count}/{after_count}]")
            
        return new_value

    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡πà‡∏≤ TIME/STRIP...")
    df_merged['TIME/STRIP'] = df_merged.apply(assign_avg, axis=1)
    
    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà: ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    print("üìä ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î...")
    df_merged['Before_Outlier'] = df_merged.apply(assign_before_count, axis=1)
    df_merged['After_Outlier'] = df_merged.apply(assign_after_count, axis=1)
    
    print("‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢")
    return df_merged

def run(input_path, output_dir):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå LOGVIEW
    """
    print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• LOGVIEW")
    print(f"üìÅ Input: {input_path}")
    print(f"üìÅ Output: {output_dir}")
    
    # 1. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå input ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå .xlsx ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå input...")
    before_files = set(f for f in os.listdir(output_dir) if f.lower().endswith('.xlsx'))
    
    process_multiple_files_complete(input_path, output_dir)
    
    after_files = set(f for f in os.listdir(output_dir) if f.lower().endswith('.xlsx'))
    new_files = list(after_files - before_files)
    
    if not new_files:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx ‡πÉ‡∏´‡∏°‡πà")
        return

    print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà {len(new_files)} ‡πÑ‡∏ü‡∏•‡πå")

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á summary DataFrame
    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á summary...")
    summary_df = summarize_sec_strip(output_dir, new_files)
    print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• summary: {summary_df.shape}")
    
    # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå package
    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå package...")
    package_path = os.path.join(BASE_DIR, "..", "Upload", "export package and frame stock Rev.03.xlsx")
    package_path = os.path.abspath(package_path)
    
    if not os.path.exists(package_path):
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå export package and frame stock Rev.03.xlsx ‡πÉ‡∏ô Upload")
        return
    
    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Summary.csv ‡∏î‡πâ‡∏ß‡∏¢ timestamp
    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_csv = os.path.join(output_dir, f"Summary_{timestamp}.csv")
    
    # ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ group_and_average_across_frames_unique_frame ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    final_df = analyze_and_export_csv_from_df(summary_df, package_path, output_csv)
    
    print(f"üéâ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    print(f"üìä ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {output_csv}")
    print(f"üìà ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {final_df.shape[0]} ‡πÅ‡∏ñ‡∏ß")




